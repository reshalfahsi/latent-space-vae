{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsgxaylgm7K2"
      },
      "source": [
        "# **Exploring Latent Space via VAE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EOu2lnPrUfq"
      },
      "source": [
        "## **Important Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqmENC3grZr1"
      },
      "source": [
        "### **Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxcy9IlXrnxl"
      },
      "outputs": [],
      "source": [
        "!curl -LsSf https://astral.sh/uv/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGRwJdsxrr5B"
      },
      "outputs": [],
      "source": [
        "!uv pip install -q --no-cache-dir --system lightning torchmetrics umap-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8uEVP5SraPQ"
      },
      "source": [
        "### **Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXS2emVKraTx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import lightning as L\n",
        "from lightning.pytorch import Trainer, seed_everything\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "\n",
        "from torchmetrics.image import LearnedPerceptualImagePatchSimilarity\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import median_abs_deviation\n",
        "\n",
        "import umap\n",
        "import math\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import imageio\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import shutil\n",
        "import urllib.request\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "from IPython.display import Image as IImage\n",
        "from IPython.display import display\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXUu_mSlr4Va"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['axes.facecolor'] = 'lightgray'\n",
        "plt.rcParams['mathtext.fontset'] = 'cm'\n",
        "plt.rcParams['font.family'] = 'STIXGeneral'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvgiDOUQraYV"
      },
      "source": [
        "## **Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9xbjfOWrac2"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"experiment\", exist_ok=True)\n",
        "os.makedirs(\"experiment/training\", exist_ok=True)\n",
        "os.makedirs(\"experiment/dataset\", exist_ok=True)\n",
        "os.makedirs(\"experiment/model\", exist_ok=True)\n",
        "EXPERIMENT_DIR = \"experiment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3uFi1g1s4I4"
      },
      "outputs": [],
      "source": [
        "H_DIM = 256\n",
        "Z_DIM = 64\n",
        "BATCH_SIZE = 128\n",
        "MAX_EPOCH = 16\n",
        "LEARNING_RATE = 3.1e-4\n",
        "KLD_WEIGHT = 0.0249673\n",
        "SCHEDULER_GAMMA = 0.978654321\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAX_SAMPLE = 23140"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYsxaeRFxt3Q"
      },
      "outputs": [],
      "source": [
        "SEED = int(np.random.randint(2147483647))\n",
        "print(f\"Random seed: {SEED}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN-YpDFhx9JW"
      },
      "outputs": [],
      "source": [
        "METRIC_TO_MONITOR = \"val_lpips\"\n",
        "METRIC_MODE = \"min\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gob0Zo7Kragj"
      },
      "source": [
        "## **Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vajncgnyyXn"
      },
      "source": [
        "### **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrf8aVdTy0oq"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 128\n",
        "IMAGE_TRANSFORM = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            (0.5, 0.5, 0.5),\n",
        "            (0.5, 0.5, 0.5)\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBVjh9J4rakv"
      },
      "outputs": [],
      "source": [
        "class CelebADataModule(L.LightningDataModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.split = {\n",
        "            \"train\": \"all\",\n",
        "            \"val\": \"valid\",\n",
        "        }\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.transform = IMAGE_TRANSFORM\n",
        "        self.dataset = datasets.CelebA\n",
        "        self.url_download = (\n",
        "            \"https://github.com/reshalfahsi/latent-space-vae\"\n",
        "            \"/releases/download/dataset\"\n",
        "        )\n",
        "        self.download_files = [\n",
        "            \"identity_CelebA.txt\",\n",
        "            \"img_align_celeba.zip\",\n",
        "            \"list_attr_celeba.txt\",\n",
        "            \"list_bbox_celeba.txt\",\n",
        "            \"list_eval_partition.txt\",\n",
        "            \"list_landmarks_align_celeba.txt\",\n",
        "        ]\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if not os.path.exists(\n",
        "            f\"{EXPERIMENT_DIR}/dataset/celeba/img_align_celeba.zip\"\n",
        "        ):\n",
        "            os.makedirs(f\"{EXPERIMENT_DIR}/dataset/celeba\", exist_ok=True)\n",
        "            for filename in self.download_files:\n",
        "                urllib.request.urlretrieve(\n",
        "                    f\"{self.url_download}/{filename}\",\n",
        "                    f\"{EXPERIMENT_DIR}/dataset/celeba/{filename}\"\n",
        "                )\n",
        "            self.dataset(\n",
        "                root=f\"{EXPERIMENT_DIR}/dataset\",\n",
        "                split=self.split['train'],\n",
        "                transform=self.transform,\n",
        "                download=True,\n",
        "            )\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.train_dataset = self.dataset(\n",
        "                root=f\"{EXPERIMENT_DIR}/dataset\",\n",
        "                split=self.split['train'],\n",
        "                transform=self.transform,\n",
        "                download=False,\n",
        "            )\n",
        "            if stage == \"fit\":\n",
        "                self.val_dataset = self.dataset(\n",
        "                    root=f\"{EXPERIMENT_DIR}/dataset\",\n",
        "                    split=self.split['val'],\n",
        "                    transform=self.transform,\n",
        "                    download=False,\n",
        "                )\n",
        "\n",
        "    def generation_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJZBubKVrapc"
      },
      "source": [
        "### **Load**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-9bU5oQycJx"
      },
      "outputs": [],
      "source": [
        "DATASET = CelebADataModule()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ghotjstra3c"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6is2YG9JpRk"
      },
      "source": [
        "### **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeXfqmu6ra7V"
      },
      "outputs": [],
      "source": [
        "class AvgMeter(object):\n",
        "    def __init__(self, num=40):\n",
        "        self.num = num\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.scores = []\n",
        "\n",
        "    def update(self, val):\n",
        "        self.scores.append(val)\n",
        "\n",
        "    def show(self):\n",
        "        out = torch.mean(\n",
        "            torch.stack(\n",
        "                self.scores[np.maximum(len(self.scores)-self.num, 0):]\n",
        "            )\n",
        "        )\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W3xibzwra_U"
      },
      "source": [
        "### **VAE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEtyZCycrbD_"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        h_dim=H_DIM,\n",
        "        z_dim=Z_DIM,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        in_channels=3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels, h_dim // 8, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.SiLU(),\n",
        "            ####################################################################\n",
        "            nn.Conv2d(\n",
        "                h_dim // 8, h_dim // 4, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.SiLU(),\n",
        "            ####################################################################\n",
        "            nn.Conv2d(\n",
        "                h_dim // 4, h_dim // 2, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.SiLU(),\n",
        "            ####################################################################\n",
        "            nn.Conv2d(\n",
        "                h_dim // 2, h_dim, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.SiLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Re-parameterization\n",
        "        self.fc_mu = nn.Linear(\n",
        "            h_dim * (image_size // 16) * (image_size // 16), z_dim\n",
        "        )\n",
        "        self.fc_logvar = nn.Linear(\n",
        "            h_dim * (image_size // 16) * (image_size // 16), z_dim\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.fc_decode = nn.Linear(\n",
        "            z_dim, h_dim * (image_size // 16) * (image_size // 16)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Unflatten(1, (h_dim, (image_size // 16), (image_size // 16))),\n",
        "            nn.ConvTranspose2d(\n",
        "                h_dim, h_dim // 2, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.SiLU(),\n",
        "            ####################################################################\n",
        "            nn.ConvTranspose2d(\n",
        "                h_dim // 2, h_dim // 4, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.SiLU(),\n",
        "            ####################################################################\n",
        "            nn.ConvTranspose2d(\n",
        "                h_dim // 4, h_dim // 8, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.SiLU(),\n",
        "            ####################################################################\n",
        "            nn.ConvTranspose2d(\n",
        "                h_dim // 8, in_channels, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.fc_mu(h)\n",
        "        log_var = self.fc_logvar(h)\n",
        "        return mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var / 2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.fc_decode(z)\n",
        "        # Reshape h back to convolutional layers' expected input shape\n",
        "        # The shape is (batch_size, channels, height, width)\n",
        "        # Example:\n",
        "        #   - After fc_decode, h is (batch_size, 256 * 8 * 8)\n",
        "        #   - We need to reshape it to (batch_size, 256, 8, 8)\n",
        "        # The Unflatten layer in the Sequential decoder takes care of this\n",
        "        reconst_image = self.decoder(h)\n",
        "        return reconst_image\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_reconst = self.decode(z)\n",
        "        return x_reconst, mu, log_var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvAs81A6Ki9u"
      },
      "source": [
        "### **Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM5uHGDRKmDF"
      },
      "outputs": [],
      "source": [
        "class VAELoss(nn.Module):\n",
        "    def __init__(self, kld_weight=KLD_WEIGHT):\n",
        "        super().__init__()\n",
        "        self.kld_weight = kld_weight\n",
        "\n",
        "    def forward(self, reconst_x, x, mu, log_var):\n",
        "        reconst_loss = nn.functional.mse_loss(reconst_x, x)\n",
        "        kld_loss = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        loss = reconst_loss + self.kld_weight * kld_loss\n",
        "        return loss, reconst_loss, kld_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djr6T8MArbHp"
      },
      "source": [
        "### **Wrapper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pER-Eb3GrbMc"
      },
      "outputs": [],
      "source": [
        "class VAEWrapper(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = VAE()\n",
        "        self.loss = VAELoss()\n",
        "\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.lr = LEARNING_RATE\n",
        "        self.max_epoch = MAX_EPOCH\n",
        "\n",
        "        self.val_lpips = LearnedPerceptualImagePatchSimilarity()\n",
        "\n",
        "        self.val_lpips.eval()\n",
        "\n",
        "        self.val_lpips_recorder = AvgMeter()\n",
        "\n",
        "        self.val_lpips_list = list()\n",
        "\n",
        "        self.train_loss_recorder = AvgMeter()\n",
        "        self.train_reconst_loss_recorder = AvgMeter()\n",
        "        self.train_kld_loss_recorder = AvgMeter()\n",
        "\n",
        "        self.train_loss = list()\n",
        "        self.train_reconst_loss = list()\n",
        "        self.train_kld_loss = list()\n",
        "\n",
        "        self.sanity_check_counter = 1\n",
        "\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        reconst_images, _, _ = self.model(x)\n",
        "        return reconst_images\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, _ = batch\n",
        "\n",
        "        reconst_images, mu, log_var = self.model(images)\n",
        "        loss, reconst_loss, kld_loss = self.loss(\n",
        "            reconst_images, images, mu, log_var\n",
        "        )\n",
        "\n",
        "        opt = self.optimizers()\n",
        "        opt.zero_grad()\n",
        "        self.manual_backward(loss)\n",
        "        opt.step()\n",
        "\n",
        "        self.log(\"train_loss\", loss.data.cpu(), prog_bar=True)\n",
        "        self.train_loss_recorder.update(loss.data.cpu())\n",
        "\n",
        "        self.log(\"train_reconst_loss\", reconst_loss.data.cpu(), prog_bar=True)\n",
        "        self.train_reconst_loss_recorder.update(reconst_loss.data.cpu())\n",
        "\n",
        "        self.log(\"train_kld_loss\", kld_loss.data.cpu(), prog_bar=True)\n",
        "        self.train_kld_loss_recorder.update(kld_loss.data.cpu())\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        mean = self.train_loss_recorder.show()\n",
        "        self.train_loss.append(mean.data.cpu().numpy())\n",
        "        self.train_loss_recorder = AvgMeter()\n",
        "\n",
        "        mean = self.train_reconst_loss_recorder.show()\n",
        "        self.train_reconst_loss.append(mean.data.cpu().numpy())\n",
        "        self.train_reconst_loss_recorder = AvgMeter()\n",
        "\n",
        "        mean = self.train_kld_loss_recorder.show()\n",
        "        self.train_kld_loss.append(mean.data.cpu().numpy())\n",
        "        self.train_kld_loss_recorder = AvgMeter()\n",
        "\n",
        "        self._plot_evaluation_metrics()\n",
        "\n",
        "        scheduler = self.lr_schedulers()\n",
        "        scheduler.step()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, _ = batch\n",
        "\n",
        "        reconst_images = self(images)\n",
        "\n",
        "        if self.sanity_check_counter == 0:\n",
        "            self.val_lpips.update(reconst_images, images)\n",
        "\n",
        "            lpips = self.val_lpips.compute()\n",
        "\n",
        "            self.log(\"val_lpips\", lpips.data.cpu(), prog_bar=True)\n",
        "            self.val_lpips_recorder.update(lpips.data.cpu())\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if self.sanity_check_counter == 0:\n",
        "            mean = self.val_lpips_recorder.show()\n",
        "            self.val_lpips_list.append(mean.data.cpu().numpy())\n",
        "            self.val_lpips_recorder = AvgMeter()\n",
        "        else:\n",
        "            self.sanity_check_counter -= 1\n",
        "\n",
        "    def _plot_evaluation_metrics(self):\n",
        "        # VAE Loss\n",
        "        vae_loss_img_file = os.path.join(\n",
        "            EXPERIMENT_DIR,\n",
        "            \"training/VAE_loss_plot.png\"\n",
        "        )\n",
        "        plt.plot(self.train_loss, color=\"r\", label=\"loss\")\n",
        "        plt.plot(self.train_reconst_loss, color=\"g\", label=\"reconst_loss\")\n",
        "        plt.plot(self.train_kld_loss, color=\"b\", label=\"kld_loss\")\n",
        "        plt.title(\"VAE Loss Curves\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.savefig(vae_loss_img_file)\n",
        "        plt.clf()\n",
        "\n",
        "        # LPIPS\n",
        "        lpips_img_file = os.path.join(\n",
        "            EXPERIMENT_DIR,\n",
        "            \"training/VAE_lpips_plot.png\"\n",
        "        )\n",
        "        plt.plot(self.val_lpips_list, color=\"b\", label=\"lpips_score\")\n",
        "        plt.title(\"LPIPS Curves\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"LPIPS\")\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.savefig(lpips_img_file)\n",
        "        plt.clf()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "\n",
        "        optimizer = optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=self.lr,\n",
        "        )\n",
        "\n",
        "        scheduler = optim.lr_scheduler.ExponentialLR(\n",
        "            optimizer,\n",
        "            gamma=SCHEDULER_GAMMA,\n",
        "        )\n",
        "\n",
        "        return [optimizer], [scheduler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SgmkkA4O_ok"
      },
      "outputs": [],
      "source": [
        "MODEL = VAEWrapper\n",
        "BEST_MODEL_PATH = os.path.join(\n",
        "    EXPERIMENT_DIR,\n",
        "    \"model\",\n",
        "    \"VAE_best.ckpt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzMMsSSJrbQM"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoCWes9BrbU0"
      },
      "outputs": [],
      "source": [
        "def _train_loop():\n",
        "    seed_everything(SEED, workers=True)\n",
        "\n",
        "    print(\"\\n=================[ Variational Auto Encoder ]=================\\n\")\n",
        "\n",
        "    model = MODEL()\n",
        "\n",
        "    callbacks = list()\n",
        "\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        monitor=METRIC_TO_MONITOR,\n",
        "        dirpath=os.path.join(EXPERIMENT_DIR, \"model\"),\n",
        "        mode=METRIC_MODE,\n",
        "        filename=\"VAE_best\",\n",
        "    )\n",
        "    callbacks.append(checkpoint)\n",
        "\n",
        "    if os.path.exists(BEST_MODEL_PATH):\n",
        "        ckpt_path = BEST_MODEL_PATH\n",
        "    else:\n",
        "        ckpt_path = None\n",
        "\n",
        "    trainer = Trainer(\n",
        "        accelerator=\"auto\",\n",
        "        devices=1,\n",
        "        max_epochs=MAX_EPOCH,\n",
        "        logger=False,\n",
        "        callbacks=callbacks,\n",
        "        log_every_n_steps=5,\n",
        "    )\n",
        "    trainer.fit(model, ckpt_path=ckpt_path, datamodule=DATASET)\n",
        "\n",
        "_train_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5_WLO0oPoPe"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(\n",
        "    cv2.imread(os.path.join(EXPERIMENT_DIR, \"training/VAE_loss_plot.png\"))\n",
        ")\n",
        "cv2_imshow(\n",
        "    cv2.imread(os.path.join(EXPERIMENT_DIR, \"training/VAE_lpips_plot.png\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYeyAmjKrbYX"
      },
      "source": [
        "## **Explore the Latent Space!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kJjXyQdGrbcc"
      },
      "outputs": [],
      "source": [
        "# @title **Prepare**\n",
        "\n",
        "try:\n",
        "    DATASET.prepare_data()\n",
        "    DATASET.setup()\n",
        "    generation_loader = DATASET.generation_dataloader()\n",
        "except Exception as e:\n",
        "    print(f\"Cannot prepare data: {e}\")\n",
        "    generation_loader = None\n",
        "\n",
        "model = VAEWrapper.load_from_checkpoint(BEST_MODEL_PATH).model\n",
        "model.eval()\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJnXXqTsOJi4"
      },
      "source": [
        "### **Random Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxiEouKiOEKe"
      },
      "outputs": [],
      "source": [
        "# Generate samples\n",
        "\n",
        "sample_size = 64\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(sample_size, Z_DIM).to(DEVICE)\n",
        "    generated_images = model.decode(z)\n",
        "\n",
        "    # Then normalize\n",
        "    generated_images_norm = 0.5 * (generated_images + 1.)\n",
        "\n",
        "    # Resize the interpolated images\n",
        "    # Define the desired output size\n",
        "    output_size = (256, 256)\n",
        "    # Use torchvision.transforms.Resize to resize the images\n",
        "    resize_transform = transforms.Resize(output_size)\n",
        "    generated_images_norm = resize_transform(generated_images_norm)\n",
        "\n",
        "    save_image(\n",
        "        generated_images_norm.cpu(),\n",
        "        f\"{EXPERIMENT_DIR}/generated_images.png\",\n",
        "    )\n",
        "\n",
        "    print(\"Generated samples saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp4W-gyNOk3w"
      },
      "source": [
        "### **Visualize Sampled Image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Uh_7ow3OlIh"
      },
      "outputs": [],
      "source": [
        "display(IImage(filename=f\"{EXPERIMENT_DIR}/generated_images.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uQDHkLkxrbkW"
      },
      "outputs": [],
      "source": [
        "# @title **Pick an Attribute!**\n",
        "attr_names = [\n",
        "    \"5_o_Clock_Shadow\",\n",
        "    \"Arched_Eyebrows\",\n",
        "    \"Attractive\",\n",
        "    \"Bags_Under_Eyes\",\n",
        "    \"Bald\",\n",
        "    \"Bangs\",\n",
        "    \"Big_Lips\",\n",
        "    \"Big_Nose\",\n",
        "    \"Black_Hair\",\n",
        "    \"Blond_Hair\",\n",
        "    \"Blurry\",\n",
        "    \"Brown_Hair\",\n",
        "    \"Bushy_Eyebrows\",\n",
        "    \"Chubby\",\n",
        "    \"Double_Chin\",\n",
        "    \"Eyeglasses\",\n",
        "    \"Goatee\",\n",
        "    \"Gray_Hair\",\n",
        "    \"Heavy_Makeup\",\n",
        "    \"High_Cheekbones\",\n",
        "    \"Male\",\n",
        "    \"Mouth_Slightly_Open\",\n",
        "    \"Mustache\",\n",
        "    \"Narrow_Eyes\",\n",
        "    \"No_Beard\",\n",
        "    \"Oval_Face\",\n",
        "    \"Pale_Skin\",\n",
        "    \"Pointy_Nose\",\n",
        "    \"Receding_Hairline\",\n",
        "    \"Rosy_Cheeks\",\n",
        "    \"Sideburns\",\n",
        "    \"Smiling\",\n",
        "    \"Straight_Hair\",\n",
        "    \"Wavy_Hair\",\n",
        "    \"Wearing_Earrings\",\n",
        "    \"Wearing_Hat\",\n",
        "    \"Wearing_Lipstick\",\n",
        "    \"Wearing_Necklace\",\n",
        "    \"Wearing_Necktie\",\n",
        "    \"Young\"\n",
        "]\n",
        "attr_dict = {\n",
        "    \"5_o_Clock_Shadow\": 0,\n",
        "    \"Arched_Eyebrows\": 1,\n",
        "    \"Attractive\": 2,\n",
        "    \"Bags_Under_Eyes\": 3,\n",
        "    \"Bald\": 4,\n",
        "    \"Bangs\": 5,\n",
        "    \"Big_Lips\": 6,\n",
        "    \"Big_Nose\": 7,\n",
        "    \"Black_Hair\": 8,\n",
        "    \"Blond_Hair\": 9,\n",
        "    \"Blurry\": 10,\n",
        "    \"Brown_Hair\": 11,\n",
        "    \"Bushy_Eyebrows\": 12,\n",
        "    \"Chubby\": 13,\n",
        "    \"Double_Chin\": 14,\n",
        "    \"Eyeglasses\": 15,\n",
        "    \"Goatee\": 16,\n",
        "    \"Gray_Hair\": 17,\n",
        "    \"Heavy_Makeup\": 18,\n",
        "    \"High_Cheekbones\": 19,\n",
        "    \"Male\": 20,\n",
        "    \"Mouth_Slightly_Open\": 21,\n",
        "    \"Mustache\": 22,\n",
        "    \"Narrow_Eyes\": 23,\n",
        "    \"No_Beard\": 24,\n",
        "    \"Oval_Face\": 25,\n",
        "    \"Pale_Skin\": 26,\n",
        "    \"Pointy_Nose\": 27,\n",
        "    \"Receding_Hairline\": 28,\n",
        "    \"Rosy_Cheeks\": 29,\n",
        "    \"Sideburns\": 30,\n",
        "    \"Smiling\": 31,\n",
        "    \"Straight_Hair\": 32,\n",
        "    \"Wavy_Hair\": 33,\n",
        "    \"Wearing_Earrings\": 34,\n",
        "    \"Wearing_Hat\": 35,\n",
        "    \"Wearing_Lipstick\": 36,\n",
        "    \"Wearing_Necklace\": 37,\n",
        "    \"Wearing_Necktie\": 38,\n",
        "    \"Young\": 39,\n",
        "    \"Random\": 99,\n",
        "}\n",
        "attribute = \"Male\" #@param [\"5_o_Clock_Shadow\", \"Arched_Eyebrows\", \"Attractive\", \"Bags_Under_Eyes\", \"Bald\", \"Bangs\", \"Big_Lips\", \"Big_Nose\", \"Black_Hair\", \"Blond_Hair\", \"Blurry\", \"Brown_Hair\", \"Bushy_Eyebrows\", \"Chubby\", \"Double_Chin\", \"Eyeglasses\", \"Goatee\", \"Gray_Hair\", \"Heavy_Makeup\", \"High_Cheekbones\", \"Male\", \"Mouth_Slightly_Open\", \"Mustache\", \"Narrow_Eyes\", \"No_Beard\", \"Oval_Face\", \"Pale_Skin\", \"Pointy_Nose\", \"Receding_Hairline\", \"Rosy_Cheeks\", \"Sideburns\", \"Smiling\", \"Straight_Hair\", \"Wavy_Hair\", \"Wearing_Earrings\", \"Wearing_Hat\", \"Wearing_Lipstick\", \"Wearing_Necklace\", \"Wearing_Necktie\", \"Young\", \"Random\"]\n",
        "positive_attribute_index = attr_dict[attribute]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhLwbXzhIz2n"
      },
      "source": [
        "### **Collect and Process Latent Vectors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycD4wss-Iz6Y"
      },
      "outputs": [],
      "source": [
        "# Collect latent vectors and corresponding labels (positive/negative)\n",
        "all_latent_vectors = list()\n",
        "all_labels = list()\n",
        "\n",
        "try:\n",
        "    print(\n",
        "        f\"Selected positive attribute: {attr_names[positive_attribute_index]}\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate(tqdm(generation_loader)):\n",
        "            images = images.to(DEVICE)\n",
        "            # Get the positive label for this batch\n",
        "            # Assuming the labels tensor has the positive attribute at\n",
        "            # positive_attribute_index\n",
        "            positive_labels = labels[:, positive_attribute_index]\n",
        "\n",
        "            # Encode images to get latent vectors\n",
        "            mu, log_var = model.encode(images)\n",
        "            # Use reparameterized z for visualization\n",
        "            z = model.reparameterize(mu, log_var)\n",
        "\n",
        "            all_latent_vectors.append(z.cpu().numpy())\n",
        "            all_labels.append(positive_labels.cpu().numpy())\n",
        "\n",
        "            # Process a reasonable number of data\n",
        "            # Adjust this number based on needs and memory\n",
        "            if i * BATCH_SIZE > MAX_SAMPLE:\n",
        "                break\n",
        "\n",
        "    all_latent_vectors = np.concatenate(all_latent_vectors, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "except Exception as e:\n",
        "    print(f\"Cannot use attr_names: {e}\")\n",
        "    all_latent_vectors = np.random.randn(MAX_SAMPLE, Z_DIM)\n",
        "    all_labels = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_FAYhf6Vqzg"
      },
      "outputs": [],
      "source": [
        "random_state = random.randint(0, np.iinfo(np.int32).max)\n",
        "reducer = umap.UMAP(\n",
        "    n_components=2,\n",
        "    n_neighbors=25,\n",
        "    min_dist=0.1,\n",
        "    random_state=random_state,\n",
        ")\n",
        "embedding_latent = reducer.fit_transform(\n",
        "    all_latent_vectors,\n",
        "    y=all_labels,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLWMPh6rVxqt"
      },
      "outputs": [],
      "source": [
        "# Find the min and max values of the UMAP embedding\n",
        "x_min, x_max = embedding_latent[:, 0].min(), embedding_latent[:, 0].max()\n",
        "y_min, y_max = embedding_latent[:, 1].min(), embedding_latent[:, 1].max()\n",
        "\n",
        "print(f\"x_min: {x_min}, x_max: {x_max}\")\n",
        "print(f\"y_min: {y_min}, y_max: {y_max}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Create a grid of points\n",
        "grid_size = 15\n",
        "grid_x = np.linspace(x_min, x_max, grid_size)\n",
        "grid_y = np.linspace(y_min, y_max, grid_size)\n",
        "\n",
        "# Generate grid points in the 2D UMAP space\n",
        "grid_points_2d = list()\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        grid_points_2d.append([grid_x[j], grid_y[i]])\n",
        "\n",
        "grid_points_2d = np.array(grid_points_2d)\n",
        "\n",
        "# Inverse transform the UMAP grid points back to the original latent space\n",
        "# This is an approximation as UMAP's inverse transform is not perfect\n",
        "# Use the trained UMAP reducer to inverse transform\n",
        "grid_points_latent = reducer.inverse_transform(grid_points_2d)\n",
        "\n",
        "# Convert the latent points to a PyTorch tensor\n",
        "grid_points_latent_tensor = torch.tensor(\n",
        "    grid_points_latent, dtype=torch.float32\n",
        ").to(DEVICE)\n",
        "\n",
        "# Decode the latent points to generate images\n",
        "with torch.no_grad():\n",
        "    generated_grid_images = model.decode(grid_points_latent_tensor)\n",
        "\n",
        "# Normalize the images\n",
        "generated_grid_images_norm = 0.5 * (generated_grid_images + 1.)\n",
        "\n",
        "# Arrange the images in a grid for visualization\n",
        "# We need to reshape the generated images to a grid of size\n",
        "# grid_size x grid_size and then stack them\n",
        "generated_grid_images_norm = generated_grid_images_norm.cpu()\n",
        "\n",
        "# Create a list of images for the grid\n",
        "image_list = list()\n",
        "for i in range(grid_size):\n",
        "    row_images = list()\n",
        "    for j in range(grid_size):\n",
        "        # The order in grid_points_2d is row by row (y then x)\n",
        "        image_index = i * grid_size + j\n",
        "        row_images.append(generated_grid_images_norm[image_index])\n",
        "    # Stack images in a row\n",
        "    image_list.append(torch.cat(row_images, dim=2)) # Stack along width (dim=2)\n",
        "\n",
        "# flipped the y-axis for proper visualization\n",
        "image_list.reverse()\n",
        "\n",
        "# Stack rows of images\n",
        "final_image_grid = torch.cat(image_list, dim=1) # Stack along height (dim=1)\n",
        "\n",
        "# Save the grid of images\n",
        "save_image(\n",
        "    final_image_grid,\n",
        "    f\"{EXPERIMENT_DIR}/latent_space_grid.png\",\n",
        ")\n",
        "\n",
        "print(\"Latent space grid of generated images saved.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2k9zbk9BCT9"
      },
      "outputs": [],
      "source": [
        "# Load the image to draw on\n",
        "image_path = os.path.join(EXPERIMENT_DIR, \"latent_space_grid.png\")\n",
        "img = cv2.imread(image_path)\n",
        "\n",
        "# Get image dimensions\n",
        "height, width, _ = img.shape\n",
        "\n",
        "# Define start and end points for the lines (relative to the image)\n",
        "# Center point is the center of the image\n",
        "center_x, center_y = width // 2, height // 2\n",
        "\n",
        "# Horizontal line: starts at left edge, ends at right edge,\n",
        "# at the vertical center\n",
        "start_point_h = (0, center_y)\n",
        "end_point_h = (width, center_y)\n",
        "\n",
        "# Vertical line: starts at top edge, ends at bottom edge,\n",
        "# at the horizontal center\n",
        "start_point_v = (center_x, 0)\n",
        "end_point_v = (center_x, height)\n",
        "\n",
        "# Define line color (BGR format) and thickness\n",
        "line_color = (0, 0, 255)  # Red color\n",
        "line_thickness = 2\n",
        "\n",
        "# Draw the horizontal line\n",
        "cv2.line(img, start_point_h, end_point_h, line_color, line_thickness)\n",
        "\n",
        "# Draw the vertical line\n",
        "cv2.line(img, start_point_v, end_point_v, line_color, line_thickness)\n",
        "\n",
        "# Calculate the intersection point\n",
        "intersection_point = (center_x, center_y)\n",
        "\n",
        "# Define text to display at the intersection\n",
        "text_to_display = \"O\"\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "font_scale = 1\n",
        "font_thickness = 2\n",
        "text_color = (255, 0, 0)\n",
        "\n",
        "# Get the size of the text\n",
        "(text_width, text_height), baseline = cv2.getTextSize(\n",
        "    text_to_display, font, font_scale, font_thickness\n",
        ")\n",
        "\n",
        "# Calculate the text position so it's centered at the intersection\n",
        "text_x = intersection_point[0] - text_width // 2\n",
        "# Adjust y based on text height\n",
        "text_y = intersection_point[1] + text_height // 2\n",
        "\n",
        "# Draw the text on the image\n",
        "cv2.putText(\n",
        "    img,\n",
        "    text_to_display,\n",
        "    (text_x, text_y),\n",
        "    font,\n",
        "    font_scale,\n",
        "    text_color,\n",
        "    font_thickness,\n",
        "    cv2.LINE_AA,\n",
        ")\n",
        "\n",
        "# Save the image with the lines\n",
        "cv2.imwrite(image_path, img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmOzxEaTIz-h"
      },
      "source": [
        "### **The 2D Latent Space Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Yb9LGoII0Bz"
      },
      "outputs": [],
      "source": [
        "# Display the generated grid image\n",
        "display(IImage(filename=f\"{EXPERIMENT_DIR}/latent_space_grid.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "smuAaxZBKCTo"
      },
      "outputs": [],
      "source": [
        "# @title **Latent Space Traversal**\n",
        "\n",
        "x_pos = 0.0 #@param {type:\"slider\", min:-1, max:1, step:1e-15}\n",
        "y_pos = 0.0 #@param {type:\"slider\", min:-1, max:1, step:1e-15}\n",
        "\n",
        "x_pos = (x_pos + 1.) / 2.\n",
        "y_pos = (y_pos + 1.) / 2.\n",
        "\n",
        "x_pos = x_pos * (x_max - x_min) + x_min\n",
        "y_pos = y_pos * (y_max - y_min) + y_min\n",
        "\n",
        "traversed_latent_vector = reducer.inverse_transform(\n",
        "    np.array([[x_pos, y_pos]])\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    traversed_image = model.decode(\n",
        "        torch.tensor(traversed_latent_vector, dtype=torch.float32).to(DEVICE)\n",
        "    )\n",
        "\n",
        "# Resize the interpolated images\n",
        "# Define the desired output size\n",
        "output_size = (256, 256)\n",
        "# Use torchvision.transforms.Resize to resize the images\n",
        "resize_transform = transforms.Resize(output_size)\n",
        "traversed_image = resize_transform(traversed_image)\n",
        "\n",
        "# Normalize image\n",
        "traversed_image = 0.5 * (traversed_image + 1.)\n",
        "\n",
        "save_image(\n",
        "    traversed_image.cpu(),\n",
        "    f\"{EXPERIMENT_DIR}/traversed_image.png\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q29yGKO9NIxn"
      },
      "source": [
        "### **Image at the Location**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXNcKH5cNITh"
      },
      "outputs": [],
      "source": [
        "display(IImage(filename=f\"{EXPERIMENT_DIR}/traversed_image.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvT0LGWCI0Fn"
      },
      "source": [
        "### **Latent Vector Interpolation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UZXr8XeI0Jd"
      },
      "outputs": [],
      "source": [
        "expected_positive_vector = None\n",
        "expected_negative_vector = None\n",
        "\n",
        "try:\n",
        "    # Separate latent vectors based on the positive label\n",
        "    positive_latent_vectors = reducer.transform(\n",
        "        all_latent_vectors[all_labels == 1]\n",
        "    )\n",
        "    negative_latent_vectors = reducer.transform(\n",
        "        all_latent_vectors[all_labels == 0]\n",
        "    )\n",
        "\n",
        "    # Calculate the expected latent vector for each group\n",
        "    expected_positive_vector = np.median(positive_latent_vectors, axis=0)\n",
        "    expected_negative_vector = np.median(negative_latent_vectors, axis=0)\n",
        "\n",
        "    # Calculate the dispersion latent vector for each group\n",
        "    dispersion_positive_vector = median_abs_deviation(\n",
        "        positive_latent_vectors, axis=0\n",
        "    )\n",
        "    dispersion_negative_vector = median_abs_deviation(\n",
        "        negative_latent_vectors, axis=0\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Expected Latent Vector for {attr_names[positive_attribute_index]}:\"\n",
        "    )\n",
        "    print(expected_positive_vector)\n",
        "    print(\n",
        "        f\"Dispersion of Latent Vector for \"\n",
        "        f\"{attr_names[positive_attribute_index]}:\"\n",
        "    )\n",
        "    print(dispersion_positive_vector)\n",
        "\n",
        "    print()\n",
        "\n",
        "    print(\n",
        "        \"Expected Latent Vector for Not \"\n",
        "        f\"{attr_names[positive_attribute_index]}:\"\n",
        "    )\n",
        "    print(expected_negative_vector)\n",
        "    print(\n",
        "        f\"Dispersion of Latent Vector for \"\n",
        "        f\"Not {attr_names[positive_attribute_index]}:\"\n",
        "    )\n",
        "    print(dispersion_negative_vector)\n",
        "\n",
        "    # Calculate the difference vector\n",
        "    difference_vector = expected_positive_vector - expected_negative_vector\n",
        "    print(\n",
        "        f\"\\nDifference Vector ({attr_names[positive_attribute_index]} - \"\n",
        "        f\"Not {attr_names[positive_attribute_index]}):\"\n",
        "    )\n",
        "    print(difference_vector)\n",
        "except Exception as e:\n",
        "    print(\n",
        "        f\"Error during latent vector calculation: {e}\\n\"\n",
        "        \"Not generating pre-generated vectors\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciEdSoAKWu49"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    expected_positive_vector = expected_positive_vector + (\n",
        "        (dispersion_positive_vector)\n",
        "        * np.random.rand(2)\n",
        "        - (dispersion_positive_vector / 2.)\n",
        "    )\n",
        "    print(f\"Compressed positive vector: {expected_positive_vector}\")\n",
        "    expected_positive_vector = reducer.inverse_transform(\n",
        "        [expected_positive_vector]\n",
        "    )[0]\n",
        "\n",
        "    expected_negative_vector = expected_negative_vector + (\n",
        "        (dispersion_negative_vector)\n",
        "        * np.random.rand(2)\n",
        "        - (dispersion_negative_vector / 2.)\n",
        "    )\n",
        "    print(f\"Compressed negative vector: {expected_negative_vector}\")\n",
        "    expected_negative_vector = reducer.inverse_transform(\n",
        "        [expected_negative_vector]\n",
        "    )[0]\n",
        "except:\n",
        "    print(\n",
        "        \"Warning: no pre-generated vectors \"\n",
        "        \"at hand. Using random vectors instead.\"\n",
        "    )\n",
        "\n",
        "    expected_positive_vector = np.random.rand(2)\n",
        "    expected_positive_vector[0] = (\n",
        "        (x_max - x_min) * expected_positive_vector[0] + x_min\n",
        "    )\n",
        "    expected_positive_vector[1] = (\n",
        "        (y_max - y_min) * expected_positive_vector[1] + y_min\n",
        "    )\n",
        "    print(f\"Compressed positive vector: {expected_positive_vector}\")\n",
        "    expected_positive_vector = reducer.inverse_transform(\n",
        "        [expected_positive_vector]\n",
        "    )[0]\n",
        "\n",
        "    expected_negative_vector = np.random.rand(2)\n",
        "    expected_negative_vector[0] = (\n",
        "        (x_max - x_min) * expected_negative_vector[0] + x_min\n",
        "    )\n",
        "    expected_negative_vector[1] = (\n",
        "        (y_max - y_min) * expected_negative_vector[1] + y_min\n",
        "    )\n",
        "    print(f\"Compressed negative vector: {expected_negative_vector}\")\n",
        "    expected_negative_vector = reducer.inverse_transform(\n",
        "        [expected_negative_vector]\n",
        "    )[0]\n",
        "\n",
        "print()\n",
        "\n",
        "print(f\"Expected Positive Vector: {expected_positive_vector}\")\n",
        "print()\n",
        "print(f\"Expected Negative Vector: {expected_negative_vector}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRL71AbKWxNA"
      },
      "outputs": [],
      "source": [
        "# Perform latent space interpolation\n",
        "num_interpolation_steps = 10  # Number of steps for interpolation\n",
        "interpolation_vectors = list()\n",
        "\n",
        "# Linearly interpolate between the two expected vectors\n",
        "for i in range(num_interpolation_steps):\n",
        "    # Interpolation factor from 0 to 1\n",
        "    alpha = i / (num_interpolation_steps - 1)\n",
        "    interpolated_vector = (\n",
        "        (1 - alpha) * expected_negative_vector\n",
        "        + alpha * expected_positive_vector\n",
        "    )\n",
        "    interpolation_vectors.append(interpolated_vector)\n",
        "\n",
        "# Convert the list of numpy arrays back to a torch tensor\n",
        "interpolation_vectors_tensor = torch.tensor(\n",
        "    np.array(interpolation_vectors), dtype=torch.float32\n",
        ").to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyV8CQL3W4wM"
      },
      "outputs": [],
      "source": [
        "# Generate images from the interpolated latent vectors\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # The decode function outputs images\n",
        "    interpolated_images = model.decode(interpolation_vectors_tensor)\n",
        "\n",
        "    # Then normalize\n",
        "    interpolated_images_norm = 0.5 * (interpolated_images + 1.)\n",
        "\n",
        "    # Resize the interpolated images\n",
        "    # Define the desired output size\n",
        "    output_size = (512, 512)\n",
        "    # Use torchvision.transforms.Resize to resize the images\n",
        "    resize_transform = transforms.Resize(output_size)\n",
        "    resized_interpolated_images = resize_transform(interpolated_images_norm)\n",
        "\n",
        "    # Save the interpolated images\n",
        "    save_image(\n",
        "        resized_interpolated_images.cpu(),\n",
        "        f\"{EXPERIMENT_DIR}/interpolated_images.png\",\n",
        "        nrow=num_interpolation_steps, # Arrange images in a single row\n",
        "        normalize=True,\n",
        "    )\n",
        "\n",
        "print(\n",
        "    f\"Interpolated images saved to {EXPERIMENT_DIR}/interpolated_images.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEVGEN8rXP2v"
      },
      "outputs": [],
      "source": [
        "# Interpolated_images is a torch tensor of shape (N, C, H, W)\n",
        "# where N is the number of images, C is channels, H is height, W is width.\n",
        "\n",
        "# Convert the tensor to a numpy array\n",
        "# Move to CPU if it's on GPU and permute dimensions for imageio (H, W, C)\n",
        "# Then normalize\n",
        "interpolated_images_norm = 0.5 * (interpolated_images + 1.)\n",
        "\n",
        "# Resize the interpolated images\n",
        "# Define the desired output size\n",
        "output_size = (512, 512)\n",
        "# Use torchvision.transforms.Resize to resize the images\n",
        "resize_transform = transforms.Resize(output_size)\n",
        "resized_interpolated_images = resize_transform(interpolated_images_norm)\n",
        "\n",
        "# Convert float values (0-1) to uint8 (0-255) for imageio\n",
        "interpolated_images_np = (\n",
        "    resized_interpolated_images.cpu().permute(0, 2, 3, 1).numpy() * 255\n",
        ").astype(np.uint8)\n",
        "\n",
        "# Save the images as a GIF\n",
        "gif_path = (\n",
        "    f\"{EXPERIMENT_DIR}/interpolated_animation.gif\"\n",
        ")\n",
        "imageio.mimsave(\n",
        "    gif_path, interpolated_images_np, loop=0, fps=1,\n",
        ")\n",
        "print(f\"GIF saved to {gif_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXugQZPCI0M6"
      },
      "source": [
        "### **Interpolation Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waJQyPy2I0RO"
      },
      "outputs": [],
      "source": [
        "display(IImage(filename=f\"{EXPERIMENT_DIR}/interpolated_images.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM_QgUBMXb_q"
      },
      "outputs": [],
      "source": [
        "# Display the GIF in the notebook\n",
        "display(IImage(open(gif_path, \"rb\").read()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5EOu2lnPrUfq",
        "aqmENC3grZr1",
        "G8uEVP5SraPQ",
        "xvgiDOUQraYV",
        "8vajncgnyyXn",
        "XJZBubKVrapc",
        "3ghotjstra3c",
        "z6is2YG9JpRk",
        "7W3xibzwra_U",
        "NvAs81A6Ki9u",
        "djr6T8MArbHp",
        "nzMMsSSJrbQM",
        "vJnXXqTsOJi4",
        "jhLwbXzhIz2n",
        "CvT0LGWCI0Fn"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
